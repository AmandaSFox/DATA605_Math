---
title: "DATA_605_Assignment_3_Fox"
author: "Amanda Fox"
date: "May 18, 2025"
format: pdf
---

## Preparation  
Load libraries: 

```{r libraries, message = FALSE, warning = FALSE}
# load libraries
library(tidyverse)
library(ggplot2)
library(scales)
library(Deriv)
library(rootSolve)


```

## Problem 1: Transportation Safety

### 1. Data Visualization:
  
*Create a scatter plot of stopping distance (dist) as a function of speed (speed). Add a regression line to the plot to visually assess the relationship.*
  
Stopping distance increases with speed in a roughly linear pattern, with a few possible outliers and more variability at higher speeds.
  
```{r load1, message = FALSE, warning = FALSE}


# Load data
data("cars")
glimpse(cars)

plt1 <- cars %>% 
ggplot(aes(x = speed, y = dist)) +
  geom_point(color = "steelblue") +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "darkred") +
  labs(title = "Stopping Distance vs Speed",
       x = "Speed (mph)", y = "Stopping Distance (ft)")

plt1
```
  
### 2. Build a Linear Model:
  
*Construct a simple linear regression model where stopping distance (dist) is the dependent variable and speed (speed) is the independent variable.* 
*Summarize the model to evaluate its coefficients, R-squared value, and p-value*
  
Stopping Distance = 3.932409 * speed - 17.58
  
R-squared = 0.6511 which indicatest that speed explains 65.11% of variation in stopping distance.
  
p-value = 1.49e-12 which indicates that this relatioship is very unlikely to be due to chance. 
  
```{r mod1, message = FALSE, warning = FALSE}

# Model
mod_dist <- lm(dist ~ speed, data = cars)

# Summarize 
summary(mod_dist)

# Add predicted values (verified new plot matches above) 
df_cars_model <- cars %>% 
  mutate(predicted_dist = predict(mod_dist))

glimpse(df_cars_model)
```
  
### 3 & 4: Model Quality Evaluation & Residual Analysis
  
*Calculate and interpret the R-squared value to assess the proportion of variance in stopping distance explained by speed.*
  
R-squared is 0.6511 so 65.11% of the variation in stopping distance is explained by speed.
  
*Perform a residual analysis to check the assumptions of the linear regression model, including linearity, homoscedasticity, independence, and normality of residuals.*
  *Plot the residuals versus fitted values to check for any patterns.*
  *Create a Q-Q plot of the residuals to assess normality.*
  *Perform a Shapiro-Wilk test for normality of residuals.*
  *Plot a histogram of residuals to further check for normality.*
  
Assumptions are generally well met. The relationship is roughly linear and residual tests below show a reasonable fit with roughly normal distribution of residuals and some heteroscedasticity. 
  
In the context of the small dataset size and visual examination of the scatterplots above, these results are acceptable.  Note that independence is assumed since the behavior of one car should not impact another.
  
  1. Residuals vs Fitted: Fan shape suggests some heteroscedasticity with increasing variance at higher speeds
  
  2. Histogram of Residuals: Broadly normally distributed with a slight right skew
  
  3. Q-Q: Follows the line generally but deviation at both ends, particularly the upper end, suggests some non-normality/outliers
  
  4. Shapiro-Wilkes test: p<0.05 suggests non-normality (null hypothesis = data is normal, which is rejected).
  
```{r resid1, message = FALSE, warning = FALSE}

# Add to dataframe
df_cars_model <- df_cars_model %>% 
  mutate(
    .fitted = fitted(mod_dist),
    .resid = resid(mod_dist),
    .std_resid = rstandard(mod_dist)
  )

glimpse(df_cars_model)

#---------------
# Diagnostic Plots**
#---------------

# Residuals vs Fitted
plot_resid <- df_cars_model %>% 
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs Fitted",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()

plot_resid

# Histogram of Residuals
plot_resid_hist <- df_cars_model %>% 
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 15, fill = "skyblue", color = "white") +
  labs(title = "Histogram of Residuals", x = "Residuals") +
  theme_minimal()

plot_resid_hist

# Q-Q
plot_qq <- df_cars_model %>% 
  ggplot(aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()

plot_qq

# shapiro-wilkes test
df_cars_model$.resid %>% 
  shapiro.test()
```

### 5. Conclusion
  
*Based on the model summary and residual analysis, determine whether the linear model is appropriate for this data. Discuss any potential violations of model assumptions and suggest improvements if necessary.*

Based on the model summary and residual analysis, the linear model is acceptable. The R-squared value is meaningful but not strong; model fit might improve by adding more predictors such as weight, tire specs, etc. 
  
Linearity, normality, and constant variance are generally met, with acceptable slight heteroscedasticity and non-normality in residuals. Independence was not formally tested but reasonable to assume since each car's stopping distance is unrelated to others. 


## Problem 2: Health Policy Analyst
  
### 1. Initial Assessment of Healthcare Expenditures and Life Expectancy
  
*Task: Create a scatterplot of LifeExp vs. TotExp to visualize the relationship between healthcare expenditures and life expectancy across countries.* 
  
```{r who1, message = FALSE, warning = FALSE}

#---------------
# Load data
#---------------

df_who_raw <- read_csv("https://raw.githubusercontent.com/AmandaSFox/DATA605_Math/main/Assignment_3/who.csv")

glimpse(df_who_raw)

#---------------
# Clean data
#---------------

# check for NAs
df_who_raw %>% 
  summarise(across(everything(), ~sum(is.na(.))))

# check for duplicate rows
nrow(df_who_raw) - nrow(distinct(df_who_raw))

# count unique values in each column
df_who_raw %>% 
  summarise(across(everything(), n_distinct))

# verify col 10 includes only NA values 
unique(df_who_raw$...10)

# verify both LifeExp columns are identical
all(df_who_raw$`LifeExp...2` == df_who_raw$`LifeExp...12`)

# drop second LifeExp and NA columns, rename LifeExp
df_who_clean <- df_who_raw %>% 
  select(- LifeExp...12,- ...10) %>%
  rename(LifeExp = `LifeExp...2`)

glimpse(df_who_clean)

#---------------
# Summary and Plot
#---------------

# summarize
summary(df_who_clean)

# Scatterplot with linear regression line
plt3 <- df_who_clean %>% 
  ggplot(aes(x = TotExp, y = LifeExp)) +
  geom_point(alpha = 0.6, color = "gray60") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  scale_x_continuous(labels = scales::comma)+
    labs(
    title = "Life Expectancy vs Total Health Expenditures",
    x = "Total Health Expenditures",
    y = "Life Expectancy"
  ) +
  theme_minimal()

plt3

```

*Run a simple linear regression with LifeExp as the dependent variable and TotExp as the independent variable (without transforming the variables).*
  
LifeExp = 64.75 + 0.00006297 × TotExp
  
For every additional $1 in total healthcare spending, life expectancy increases by about 0.000063 years or about 0.023 days above a "baseline" of 64.75 years at $0 spending.
  
*Provide and interpret the F-statistic, R-squared value, standard error, and p-values.*
  
* F-statistic and p value: 65.26 on 1 and 188 DF with p of 7.7e-14 indicates the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
* R-squared: 0.2577 indicates that total healthcare expenditure explains only 25.77% of the variance in life expectancy, which is weak
* Standard error: 9.37 indicates that average error is 9.37 years which is significant in this case as the interquartile range is 13.75 years.

*Discuss whether the assumptions of simple linear regression (linearity, independence, homoscedasticity, and normality of residuals) are met in this analysis.*

Overall the model violates all assumptions except independence (countries are independent observations so independence is assumed). 

* Linearity: The relationship is clearly non-linear in the scatterplot
* Homoscedasticity: The residual plot shows an uneven distribution of residuals around the line
* Normality: The histogram of residuals is skewed left and the qq plot varies widely around the line. The Shapiro-Wilkes small p value also indicates non normality.
  
*Discussion: Consider the implications of your findings for health policy. Are higher healthcare expenditures generally associated with longer life expectancy? What do the assumptions of the regression model suggest about the reliability of this relationship?*

Higher healthcare expenditures are positively correlated to life expectancy but the relationship is not linear. While the relationship is significant and not likely due to chance (low p value), the fit of this model is poor with a low R-squared and high standard error. 
  
To draw conclusions on which to base policy, additional work is needed such as transformations or a non-linear model.

``` {r who2, message = FALSE, warning = FALSE}

#---------------
# Model
#---------------
mod_life <- lm(LifeExp ~ TotExp, data = df_who_clean)

summary(mod_life)

#---------------
# Diagnostics
#---------------

# add predicted and residuals to df
df_who_model <- df_who_clean %>% 
  mutate(predicted_life = predict(mod_life),
         .fitted = fitted(mod_life),
         .resid = resid(mod_life),
         .std_resid = rstandard(mod_life))

glimpse(df_who_model)

# Residuals vs Fitted
plot_resid_who <- df_who_model %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted", x = "Fitted Values", y = "Residuals") +
  theme_minimal()

plot_resid_who


# Histogram of Residuals
plot_hist_who <- df_who_model %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 15, fill = "skyblue", color = "white") +
  labs(title = "Histogram of Residuals", x = "Residuals") +
  theme_minimal()

plot_hist_who

# QQ plot
plot_qq_who <- df_who_model %>%
  ggplot(aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()

plot_qq_who

# Shapiro Wilkes
shapiro.test(df_who_model$.resid)

```

### 2. Transforming Variables for a Better Fit
  
*Task: Recognizing potential non-linear relationships, transform the variables as follows:*
*Raise life expectancy to the 4.6 power (LifeExp^4.6).*
*Raise total expenditures to the 0.06 power (TotExp^0.06), which is nearly a logarithmic transformation.*
*Create a new scatterplot with the transformed variables and re-run the simple linear regression model.*

The transformed data now has a visually linear relationship on the scatterplot with a more even distribution around the linear regression line.

*Provide and interpret the F-statistic, R-squared value, standard error, and p-values for the transformed model.*
  
* F-statistic and p value: 507.7 on 1 and 188 DF with p of 2.2e-16 indicates the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
* R-squared: 0.7298 indicates that total healthcare expenditure now explains 72.98% of the variance in life expectancy, which is good
* Standard error: 90,490,000 is not readily interpretable but it is still pretty significant compared to the transformed life expectancy values (e.g. it's about 21.4% of 75^4.6).

The model has improved in fit with a higher R-square, but the standard error is still significant compared to the predicted values, so there is still room for improvement.

*Compare this model to the original model (from Question 1). Which model provides a better fit, and why?*
  
* F-statistic and p value: Already very good, they improved even more:  the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
* R-squared: Significant improvement from 0.2577 (weak fit) to 0.7298 (strong)
* Standard error: Proportionately, the standard error increased compared to typical life spans from approximately 12.5% of a typical 75 year life span to 21.4%.

The transformed model provides a significantly better fit but can still be improved. 

*Discussion: How do the transformations impact the interpretation of the relationship between healthcare spending and life expectancy? Why might the transformed model be more appropriate for policy recommendations?*

The transformations reveal a clear linear relationship and greatly improve the fit of the model, reducing unexplained variance. Prediction error however remains large, and to be interpretable, the data should be backtransformed. However, it is more appropriate for policy recommendations than the first model. 

``` {r who3, message = FALSE, warning = FALSE}

#---------------
# Transform and Plot
#---------------

df_who_transform <- df_who_clean %>% 
  mutate(LifeExp_46 = LifeExp^4.6,
         TotExp_006 = TotExp^0.06)

glimpse(df_who_transform)

# Scatterplot with linear regression line
plt4 <- df_who_transform %>% 
  ggplot(aes(x = TotExp_006, y = LifeExp_46)) +
  geom_point(alpha = 0.6, color = "gray60") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  scale_x_continuous(labels = scales::comma)+
    labs(
    title = "Transformed: Life Expectancy vs Total Health Expenditures",
    x = "Total Health Expenditures ^ 0.06",
    y = "Life Expectancy ^ 4.6"
  ) +
  theme_minimal()

plt4

#---------------
# Model
#---------------

mod_life_transform <- lm(LifeExp_46 ~ TotExp_006, data = df_who_transform)

summary(mod_life_transform)

```
## 3. Forecasting Life Expectancy Based on Transformed Expenditures
  
*Task: Using the results from the transformed model in Question 2, forecast the life expectancy for countries with the following transformed total expenditures (TotExp^0.06):*

* When TotExp^0.06 = 1.5 life expectancy is predicted to be **63.3** years
* When TotExp^0.06 = 2.5 life expectancy is predicted to be **86.5** years

*Discussion: Discuss the implications of these forecasts for countries with different levels of healthcare spending. What do these predictions suggest about the potential impact of increasing healthcare expenditures on life expectancy?*

Increasing health expenditures is expected to result in meaningfully increased life expectancy.

``` {r who4, message = FALSE, warning = FALSE}

#---------------
# Predict
#---------------

# create table with new total expense data
df_who_new_data <- tibble(TotExp_006 = c(1.5, 2.5))

# add predicted life expectancy and backtransform it
df_who_new_data <- df_who_new_data %>% 
  mutate(predicted_life_exp_46 = predict(mod_life_transform, 
                                         newdata = df_who_new_data),
         predicted_life_exp_backtransformed = predicted_life_exp_46^(1/4.6))

glimpse(df_who_new_data)

```

## 4: Interaction Effects in Multiple Regression
  
*Task: Build a multiple regression model to investigate the combined effect of the proportion of MDs and total healthcare expenditures on life expectancy. Specifically, use the model:*

$$\text{LifeExp} = b_0 + b_1 \times \text{PropMD} + b_2 \times \text{TotExp} + b_3 \times (\text{PropMD} \times \text{TotExp})$$
*Interpret the F-statistic, R-squared value, standard error, and p-values.*

* F-statistic and p value: 34.49 on 3 and 186 DF with p of 2.2e-16 indicates the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
  
* R-squared: 0.3574 indicates that total healthcare expenditure now explains 35.74% of the variance in life expectancy, better than the original untransformed linear model (0.26) but worse than the transformed model (0.73). Note that I also ran this model using the transformed data which resulted in an even worse R-squared of about 0.19, so the untransformed version was used.
  
* Standard error: 8.765 is slightly better than the other two models but still significant.
  
*Evaluate the interaction term (PropMD x TotExp). What does this interaction tell us about the relationship between the number of MDs, healthcare spending, and life expectancy?*
  
The interaction coefficient is highly significant and negative, while the PropMD and TotExp separately are also highly significant and positive. So increasing either has a positive impact, but oddly combining them has a smaller effect. 
  
This may mean that in a country that has a lot of docs or spending already, increasing the other variable has less effect than it otherwise would. For example, if a country already has lots of doctors, extra spending may go toward nicer clinics or equipment and have less impact on life expectancy.
  
``` {r who5, message = FALSE, warning = FALSE}

# new linear model with PropMD, TotExp, and the interaction
mod_life_interaction <- lm(LifeExp ~ PropMD + TotExp + PropMD:TotExp, 
                           data = df_who_clean)

summary(mod_life_interaction)

```

## 5: Forecasting Life Expectancy with Interaction Terms

*Task: Using the multiple regression model from Question 4, forecast the life expectancy for a country where:*

*The proportion of MDs is 0.03 (PropMD = 0.03).*
*The total healthcare expenditure is 14 (TotExp = 14).*
  
The predicted life expectancy in the above scenario is **107.7**
  
*Discussion: Does this forecast seem realistic? Why or why not? Consider both the potential strengths and limitations of using this model for forecasting in real-world policy settings.*

The forecast is not realistic: the average life expectancy of a population in the real world is extremely unlikely to be nearly 108 years. Nearly all individuals in a real-world population expire before 108.
  
However, the scenario itself is very extreme and unrealistic: the country would have nearly the lowest expenditure in the world but also nearly the highest proportion of physicians. Such a poor country could not train and support so many physicians with that level of expenditure. 

In a real world setting, models can extrapolate beyond what is realistic or even possible in the real world, and so care should be taken especially at extremes like this.

``` {r who6, message = FALSE, warning = FALSE}

#---------------
# Predict
#---------------

# create table with new total expense data
df_who_new_data2 <- tibble(TotExp = 14,
                           PropMD = 0.03)

# add predicted life expectancy and backtransform it
df_who_new_data2 <- df_who_new_data2 %>% 
  mutate(predicted_life_exp = predict(mod_life_interaction, 
                                         newdata = df_who_new_data2))

glimpse(df_who_new_data2)

# original dataset for context
# summarize
df_who_clean %>% 
  select(c(PropMD,TotExp)) %>% 
  summary()


```
## Problem 3-Retail Company Analyst
  
### 1. Inventory Cost
  
*Scenario: A retail company is planning its inventory strategy for the upcoming year. They expect to sell 110 units of a high-demand product. The storage cost is $3.75 per unit per year, and there is a fixed ordering cost of $8.25 per order. The company wants to minimize its total inventory cost.*

*Task: Using calculus, determine the optimal lot size (the number of units to order each time) and the number of orders the company should place per year to minimize total inventory costs. Assume that the total cost function is given by:*

$$ C(Q) = \frac{D}{Q} \cdot S + \frac{Q}{2} \cdot H$$
*D is the total demand (110 units).*
  
*Q is the order quantity.*
  
*S is the fixed ordering cost per order ($8.25).*
  
*H is the holding cost per unit per year ($3.75).*
  
To minimize cost, take the derivative of C(Q) (cost with respect to quantity), set it equal to zero, and solve for Q.
  
**Optimal lot size: 22**
  
**Optimal orders/year: 5**
  
``` {r retail1, message = FALSE, warning = FALSE}

D <- 110  # demand
S <- 8.25 # cost/order
H <- 3.75 # holding cost/year

# cost function
C <- function(Q) {(D/Q) * S + (Q/2) * H}

# derivative of C(Q) with respect to Q using Deriv package
C_prime <- Deriv(C, "Q")

# solve C'(Q) = 0 for optimal Q using rootSolve package
optimal_Q <- uniroot.all(C_prime, c(0.01, 1000))  # Avoid zero division
optimal_Q

# orders per year
optimal_order <- D/optimal_Q
optimal_order

# minimized cost
C(optimal_Q)

```
  
###2. Revenue Maximization
  
*Scenario: A company is running an online advertising campaign. The effectiveness of the campaign, in terms of revenue generated per day, is modeled by the function:*
  
$$R(t) = -3150t^{-4} - 220t + 6530$$
  
*R(t) represents the revenue in dollars after t days of the campaign.*
  
*Task: Determine the time t at which the revenue is maximized by finding the critical points of the revenue function and determining which point provides the maximum value. What is the maximum revenue the company can expect from this campaign?*

To find the maximum revenue, I took the derivative of R(t) and set it equal to zero to find the critical points, then found the second derivative and checked its sign at that critical point. It was negative, so that point was a maximum.

Maximum revenue = $5,912.09

``` {r retail2, message = FALSE, warning = FALSE}

R <- function(t) {-3150 * t^(-4) - 220 * t + 6530}

# Derivative
R_prime <- Deriv(R, "t")

# set R' = 0 and solve using rootSolve package
t_critical <- uniroot.all(R_prime, c(0.5, 10))  # Avoid t=0 to prevent division by zero

# Second derivative
R_double_prime <- Deriv(R_prime, "t")

# Evaluate second derivative at the critical point
R_double_prime(t_critical) # negative, so this is a maximum

# Max revenue
max_revenue <- R(t_critical)
max_revenue

```

## 3 Demand Area Under Curve

*Scenario: A company sells a product at a price that decreases over time according to the linear demand function:*

$$P(x) = 2x - 9.3 $$

*Where: 
*P(x) is the price in dollars, and 
*x is the quantity sold.

*Task: The company is interested in calculating the total revenue generated by this product between two quantity levels, 
x1 = 2 and x2 = 5, where the price still generates sales. Compute the area under the demand curve between these two points, representing the total revenue generated over this range.*

The area under the demand curve is **-6.90** and represents the total value or willingness to pay for the range of quantities. The function implies the company must pay customers to take the product for most of this range (x < = 4.65). For example for P(5) = 0.7 but P(2) = -5.3. 

It is possible that the signs in the function are transposed. In a demand curve, as price goes up, demand should go down, or as x goes up, P(x) should go down.
  
Transposing the signs gives us that negative slope: $$P(x) = -2x + 9.3$$
  
The area under the curve (total value or willingness to pay) is now **6.90** which aligns with positive consumer value for the range of quantities.

Now P(2) = 5.30, P(4.65) = 0, and P(5) = -.70. Theoretically when price reaches 0, demand should be infinite, but this is a limitation of a linear demand function. 

To find total revenue between x = 2 and x = 4 (avoiding a zero or negative price), the derivative of P(x)*x can be taken.

``` {r retail3, message = FALSE, warning = FALSE}

# Price function
P <- function(x) {2 * x - 9.3}

# AUC using base R 
area <- integrate(P, lower = 2, upper = 5)$value

# Print result
area

# Revised price function
P2 <- function(x) {-2 * x + 9.3}

# AUC using base R 
area2 <- integrate(P2, lower = 2, upper = 5)$value

# Print result
area2

```

## 4. Profit Optimization
  
*Scenario: A beauty supply store sells flat irons, and the profit function associated with selling x flat irons is given by:*

$$\pi(x) = x \ln (9x) - \frac{x^6}{6} $$

*Task: Use calculus to find the value of x that maximizes profit. Calculate the maximum profit that can be achieved and determine if this optimal sales level is feasible given market conditions.*

The flat irons are not a feasible item to sell in current conditions. The maximum profit is achieved at **x = 1.28** (one or two flat irons) and the **maximum profit is only $2.40**.

The term $\frac{-x^6}{6}$ increases rapidly with increases in quantity and quickly offsets any additional revenue.

``` {r retail4, message = FALSE, warning = FALSE}

# Profit function
Pi <- function(x) {x * log(9 * x) - (x^6) / 6}

# First derivative
Pi_prime <- Deriv(Pi, "x")

# Set Pi' = 0 and solve using rootSolve package
optimal_x <- uniroot.all(Pi_prime, c(0.1, 5))  # Avoid x = 0 due to log
optimal_x

# Second derivative
Pi_double_prime <- Deriv(Pi_prime, "x")

# Evaluate second derivative at the critical point
Pi_double_prime(optimal_x)  # Should be negative ⇒ maximum

# Max profit
max_profit <- Pi(optimal_x)
max_profit

```

## 5. Spending Behavior

*Scenario: A market research firm is analyzing the spending behavior of customers in a retail store. The spending behavior is modeled by the probability density function:*

$$f(x) = \frac{1}{6x} $$
 
*Where x represents spending in dollars.*

*Task: Determine whether this function is a valid probability density function over the interval [1, e^6]. If it is, calculate the probability that a customer spends between 1 and e^6 dollars.*

To determine if the function is a valid probability density function (PDF) over the interval, it must be non-negative and integrate to 1.

It is non-negative over the range because the whole set [1, e^6] is positive and $\frac{1}{6x}$ must be positive when x is positive.

It also integrates to 1 over this range, so **it is a valid PDF**.

The probability that a customer spends in that range, the entire PDF, is also **1**.

``` {r retail5, message = FALSE, warning = FALSE}

# PDF
pdf <- function(x) { 1 / (6 * x) }

# Define integration limits
pdf_a <- 1
pdf_b <- exp(6)

# Integrate to check validity
pdf_area <- integrate(pdf, lower = pdf_a, upper = pdf_b)
pdf_area$value  # Should be 1
```

## 6 Market Share Estimation

*Scenario: An electronics company is analyzing its market share over a certain period. The rate of market penetration is given by:*

$$\frac {dN}{dt} = \frac {500}{t^4 + 10} $$
*Where N(t) is the cumulative market share at time t.*

*Task: Integrate this function to find the cumulative market share N(t) after t days, given that the initial market share N(1) = 6530.* 
*What will the market share be after 10 days?*

After ten days, total cumulative market penetration will be 6597.54 or approximately **6580 customers**. The rate of change slows dramatically as t increases due to the t^4 in the denominator.

``` {r retail6, message = FALSE, warning = FALSE}

# define derivative dN/dt
dN_dt <- function(t) {500/(t^4 +10)}

# integrate from t= 1 to t = 10 (take value!)
mkt_integrated_area <- integrate(dN_dt, lower = 1, upper = 10)$value

# add initial market share at t = 1 
N_1 <- 6530
N_10 <- N_1 + mkt_integrated_area
N_10

```

