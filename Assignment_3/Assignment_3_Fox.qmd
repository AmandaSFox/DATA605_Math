---
title: "DATA_605_Assignment_3_Fox"
author: "Amanda Fox"
date: "May 18, 2025"
format: pdf
---

## Preparation  
Load libraries: 

```{r libraries, message = FALSE, warning = FALSE}
# load libraries
library(tidyverse)
library(ggplot2)
library(scales)
library(Deriv)
library(rootSolve)
library(tidyr)
library(numDeriv)
library(Rsolnp)

```

## Problem 1: Transportation Safety

### 1.1 Data Visualization:
  
*Create a scatter plot of stopping distance (dist) as a function of speed (speed). Add a regression line to the plot to visually assess the relationship.*
  
Stopping distance increases with speed in a roughly linear pattern, with a few possible outliers and more variability at higher speeds.
  
```{r load1, message = FALSE, warning = FALSE}


# Load data
data("cars")
glimpse(cars)

plt1 <- cars %>% 
ggplot(aes(x = speed, y = dist)) +
  geom_point(color = "steelblue") +
  geom_smooth(method = "lm", 
              se = FALSE, 
              color = "darkred") +
  labs(title = "Stopping Distance vs Speed",
       x = "Speed (mph)", y = "Stopping Distance (ft)")

plt1
```
  
### 1.2 Build a Linear Model:
  
*Construct a simple linear regression model where stopping distance (dist) is the dependent variable and speed (speed) is the independent variable.* 
*Summarize the model to evaluate its coefficients, R-squared value, and p-value*
  
Stopping Distance = 3.932409 * speed - 17.58
  
R-squared = 0.6511 which indicatest that speed explains 65.11% of variation in stopping distance.
  
p-value = 1.49e-12 which indicates that this relatioship is very unlikely to be due to chance. 
  
```{r mod1, message = FALSE, warning = FALSE}

# Model
mod_dist <- lm(dist ~ speed, data = cars)

# Summarize 
summary(mod_dist)

# Add predicted values (verified new plot matches above) 
df_cars_model <- cars %>% 
  mutate(predicted_dist = predict(mod_dist))

glimpse(df_cars_model)
```
  
### 1.3 & 1.4: Model Quality Evaluation & Residual Analysis
  
*Calculate and interpret the R-squared value to assess the proportion of variance in stopping distance explained by speed.*
  
R-squared is 0.6511 so 65.11% of the variation in stopping distance is explained by speed.
  
*Perform a residual analysis to check the assumptions of the linear regression model, including linearity, homoscedasticity, independence, and normality of residuals.*
  *Plot the residuals versus fitted values to check for any patterns.*
  *Create a Q-Q plot of the residuals to assess normality.*
  *Perform a Shapiro-Wilk test for normality of residuals.*
  *Plot a histogram of residuals to further check for normality.*
  
Assumptions are generally well met. The relationship is roughly linear and residual tests below show a reasonable fit with roughly normal distribution of residuals and some heteroscedasticity. 
  
In the context of the small dataset size and visual examination of the scatterplots above, these results are acceptable.  Note that independence is assumed since the behavior of one car should not impact another.
  
  1. Residuals vs Fitted: Fan shape suggests some heteroscedasticity with increasing variance at higher speeds
  
  2. Histogram of Residuals: Broadly normally distributed with a slight right skew
  
  3. Q-Q: Follows the line generally but deviation at both ends, particularly the upper end, suggests some non-normality/outliers
  
  4. Shapiro-Wilkes test: p<0.05 suggests non-normality (null hypothesis = data is normal, which is rejected).
  
```{r resid1, message = FALSE, warning = FALSE}

# Add to dataframe
df_cars_model <- df_cars_model %>% 
  mutate(
    .fitted = fitted(mod_dist),
    .resid = resid(mod_dist),
    .std_resid = rstandard(mod_dist)
  )

glimpse(df_cars_model)

#---------------
# Diagnostic Plots**
#---------------

# Residuals vs Fitted
plot_resid <- df_cars_model %>% 
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs Fitted",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()

plot_resid

# Histogram of Residuals
plot_resid_hist <- df_cars_model %>% 
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 15, fill = "skyblue", color = "white") +
  labs(title = "Histogram of Residuals", x = "Residuals") +
  theme_minimal()

plot_resid_hist

# Q-Q
plot_qq <- df_cars_model %>% 
  ggplot(aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()

plot_qq

# shapiro-wilkes test
df_cars_model$.resid %>% 
  shapiro.test()
```

### 1.5 Conclusion
  
*Based on the model summary and residual analysis, determine whether the linear model is appropriate for this data. Discuss any potential violations of model assumptions and suggest improvements if necessary.*

Based on the model summary and residual analysis, the linear model is acceptable. The R-squared value is meaningful but not strong; model fit might improve by adding more predictors such as weight, tire specs, etc. 
  
Linearity, normality, and constant variance are generally met, with acceptable slight heteroscedasticity and non-normality in residuals. Independence was not formally tested but reasonable to assume since each car's stopping distance is unrelated to others. 


## Problem 2: Health Policy Analyst
  
### 2.1 Initial Assessment of Healthcare Expenditures and Life Expectancy
  
*Task: Create a scatterplot of LifeExp vs. TotExp to visualize the relationship between healthcare expenditures and life expectancy across countries.* 
  
```{r who1, message = FALSE, warning = FALSE}

#---------------
# Load data
#---------------

df_who_raw <- read_csv("https://raw.githubusercontent.com/AmandaSFox/DATA605_Math/main/Assignment_3/who.csv")

glimpse(df_who_raw)

#---------------
# Clean data
#---------------

# check for NAs
df_who_raw %>% 
  summarise(across(everything(), ~sum(is.na(.))))

# check for duplicate rows
nrow(df_who_raw) - nrow(distinct(df_who_raw))

# count unique values in each column
df_who_raw %>% 
  summarise(across(everything(), n_distinct))

# verify col 10 includes only NA values 
unique(df_who_raw$...10)

# verify both LifeExp columns are identical
all(df_who_raw$`LifeExp...2` == df_who_raw$`LifeExp...12`)

# drop second LifeExp and NA columns, rename LifeExp
df_who_clean <- df_who_raw %>% 
  select(- LifeExp...12,- ...10) %>%
  rename(LifeExp = `LifeExp...2`)

glimpse(df_who_clean)

#---------------
# Summary and Plot
#---------------

# summarize
summary(df_who_clean)

# Scatterplot with linear regression line
plt3 <- df_who_clean %>% 
  ggplot(aes(x = TotExp, y = LifeExp)) +
  geom_point(alpha = 0.6, color = "gray60") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  scale_x_continuous(labels = scales::comma)+
    labs(
    title = "Life Expectancy vs Total Health Expenditures",
    x = "Total Health Expenditures",
    y = "Life Expectancy"
  ) +
  theme_minimal()

plt3

```

*Run a simple linear regression with LifeExp as the dependent variable and TotExp as the independent variable (without transforming the variables).*
  
LifeExp = 64.75 + 0.00006297 Ã— TotExp
  
For every additional $1 in total healthcare spending, life expectancy increases by about 0.000063 years or about 0.023 days above a "baseline" of 64.75 years at $0 spending.
  
*Provide and interpret the F-statistic, R-squared value, standard error, and p-values.*
  
* F-statistic and p value: 65.26 on 1 and 188 DF with p of 7.7e-14 indicates the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
* R-squared: 0.2577 indicates that total healthcare expenditure explains only 25.77% of the variance in life expectancy, which is weak
* Standard error: 9.37 indicates that average error is 9.37 years which is significant in this case as the interquartile range is 13.75 years.

*Discuss whether the assumptions of simple linear regression (linearity, independence, homoscedasticity, and normality of residuals) are met in this analysis.*

Overall the model violates all assumptions except independence (countries are independent observations so independence is assumed). 

* Linearity: The relationship is clearly non-linear in the scatterplot
* Homoscedasticity: The residual plot shows an uneven distribution of residuals around the line
* Normality: The histogram of residuals is skewed left and the qq plot varies widely around the line. The Shapiro-Wilkes small p value also indicates non normality.
  
*Discussion: Consider the implications of your findings for health policy. Are higher healthcare expenditures generally associated with longer life expectancy? What do the assumptions of the regression model suggest about the reliability of this relationship?*

Higher healthcare expenditures are positively correlated to life expectancy but the relationship is not linear. While the relationship is significant and not likely due to chance (low p value), the fit of this model is poor with a low R-squared and high standard error. 
  
To draw conclusions on which to base policy, additional work is needed such as transformations or a non-linear model.

``` {r who2, message = FALSE, warning = FALSE}

#---------------
# Model
#---------------
mod_life <- lm(LifeExp ~ TotExp, data = df_who_clean)

summary(mod_life)

#---------------
# Diagnostics
#---------------

# add predicted and residuals to df
df_who_model <- df_who_clean %>% 
  mutate(predicted_life = predict(mod_life),
         .fitted = fitted(mod_life),
         .resid = resid(mod_life),
         .std_resid = rstandard(mod_life))

glimpse(df_who_model)

# Residuals vs Fitted
plot_resid_who <- df_who_model %>%
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.6) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs Fitted", x = "Fitted Values", y = "Residuals") +
  theme_minimal()

plot_resid_who


# Histogram of Residuals
plot_hist_who <- df_who_model %>%
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 15, fill = "skyblue", color = "white") +
  labs(title = "Histogram of Residuals", x = "Residuals") +
  theme_minimal()

plot_hist_who

# QQ plot
plot_qq_who <- df_who_model %>%
  ggplot(aes(sample = .resid)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()

plot_qq_who

# Shapiro Wilkes
shapiro.test(df_who_model$.resid)

```

### 2.2 Transforming Variables for a Better Fit
  
*Task: Recognizing potential non-linear relationships, transform the variables as follows:*
*Raise life expectancy to the 4.6 power (LifeExp^4.6).*
*Raise total expenditures to the 0.06 power (TotExp^0.06), which is nearly a logarithmic transformation.*
*Create a new scatterplot with the transformed variables and re-run the simple linear regression model.*

The transformed data now has a visually linear relationship on the scatterplot with a more even distribution around the linear regression line.

*Provide and interpret the F-statistic, R-squared value, standard error, and p-values for the transformed model.*
  
* F-statistic and p value: 507.7 on 1 and 188 DF with p of 2.2e-16 indicates the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
* R-squared: 0.7298 indicates that total healthcare expenditure now explains 72.98% of the variance in life expectancy, which is good
* Standard error: 90,490,000 is not readily interpretable but it is still pretty significant compared to the transformed life expectancy values (e.g. it's about 21.4% of 75^4.6).

The model has improved in fit with a higher R-square, but the standard error is still significant compared to the predicted values, so there is still room for improvement.

*Compare this model to the original model (from Question 1). Which model provides a better fit, and why?*
  
* F-statistic and p value: Already very good, they improved even more:  the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
* R-squared: Significant improvement from 0.2577 (weak fit) to 0.7298 (strong)
* Standard error: Proportionately, the standard error increased compared to typical life spans from approximately 12.5% of a typical 75 year life span to 21.4%.

The transformed model provides a significantly better fit but can still be improved. 

*Discussion: How do the transformations impact the interpretation of the relationship between healthcare spending and life expectancy? Why might the transformed model be more appropriate for policy recommendations?*

The transformations reveal a clear linear relationship and greatly improve the fit of the model, reducing unexplained variance. Prediction error however remains large, and to be interpretable, the data should be backtransformed. However, it is more appropriate for policy recommendations than the first model. 

``` {r who3, message = FALSE, warning = FALSE}

#---------------
# Transform and Plot
#---------------

df_who_transform <- df_who_clean %>% 
  mutate(LifeExp_46 = LifeExp^4.6,
         TotExp_006 = TotExp^0.06)

glimpse(df_who_transform)

# Scatterplot with linear regression line
plt4 <- df_who_transform %>% 
  ggplot(aes(x = TotExp_006, y = LifeExp_46)) +
  geom_point(alpha = 0.6, color = "gray60") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  scale_x_continuous(labels = scales::comma)+
    labs(
    title = "Transformed: Life Expectancy vs Total Health Expenditures",
    x = "Total Health Expenditures ^ 0.06",
    y = "Life Expectancy ^ 4.6"
  ) +
  theme_minimal()

plt4

#---------------
# Model
#---------------

mod_life_transform <- lm(LifeExp_46 ~ TotExp_006, data = df_who_transform)

summary(mod_life_transform)

```
### 2.3 Forecasting Life Expectancy Based on Transformed Expenditures
  
*Task: Using the results from the transformed model in Question 2, forecast the life expectancy for countries with the following transformed total expenditures (TotExp^0.06):*

* When TotExp^0.06 = 1.5 life expectancy is predicted to be **63.3** years
* When TotExp^0.06 = 2.5 life expectancy is predicted to be **86.5** years

*Discussion: Discuss the implications of these forecasts for countries with different levels of healthcare spending. What do these predictions suggest about the potential impact of increasing healthcare expenditures on life expectancy?*

Increasing health expenditures is expected to result in meaningfully increased life expectancy.

``` {r who4, message = FALSE, warning = FALSE}

#---------------
# Predict
#---------------

# create table with new total expense data
df_who_new_data <- tibble(TotExp_006 = c(1.5, 2.5))

# add predicted life expectancy and backtransform it
df_who_new_data <- df_who_new_data %>% 
  mutate(predicted_life_exp_46 = predict(mod_life_transform, 
                                         newdata = df_who_new_data),
         predicted_life_exp_backtransformed = predicted_life_exp_46^(1/4.6))

glimpse(df_who_new_data)

```

### 2.4 Interaction Effects in Multiple Regression
  
*Task: Build a multiple regression model to investigate the combined effect of the proportion of MDs and total healthcare expenditures on life expectancy. Specifically, use the model:*

$$\text{LifeExp} = b_0 + b_1 \times \text{PropMD} + b_2 \times \text{TotExp} + b_3 \times (\text{PropMD} \times \text{TotExp})$$
*Interpret the F-statistic, R-squared value, standard error, and p-values.*

* F-statistic and p value: 34.49 on 3 and 186 DF with p of 2.2e-16 indicates the relationship between total expenditure and life expectancy is extremely unlikely to be due to chance
  
* R-squared: 0.3574 indicates that total healthcare expenditure now explains 35.74% of the variance in life expectancy, better than the original untransformed linear model (0.26) but worse than the transformed model (0.73). Note that I also ran this model using the transformed data which resulted in an even worse R-squared of about 0.19, so the untransformed version was used.
  
* Standard error: 8.765 is slightly better than the other two models but still significant.
  
*Evaluate the interaction term (PropMD x TotExp). What does this interaction tell us about the relationship between the number of MDs, healthcare spending, and life expectancy?*
  
The interaction coefficient is highly significant and negative, while the PropMD and TotExp separately are also highly significant and positive. So increasing either has a positive impact, but oddly combining them has a smaller effect. 
  
This may mean that in a country that has a lot of docs or spending already, increasing the other variable has less effect than it otherwise would. For example, if a country already has lots of doctors, extra spending may go toward nicer clinics or equipment and have less impact on life expectancy.
  
``` {r who5, message = FALSE, warning = FALSE}

# new linear model with PropMD, TotExp, and the interaction
mod_life_interaction <- lm(LifeExp ~ PropMD + TotExp + PropMD:TotExp, 
                           data = df_who_clean)

summary(mod_life_interaction)

```

### 2.5 Forecasting Life Expectancy with Interaction Terms

*Task: Using the multiple regression model from Question 4, forecast the life expectancy for a country where:*

*The proportion of MDs is 0.03 (PropMD = 0.03).*
*The total healthcare expenditure is 14 (TotExp = 14).*
  
The predicted life expectancy in the above scenario is **107.7**
  
*Discussion: Does this forecast seem realistic? Why or why not? Consider both the potential strengths and limitations of using this model for forecasting in real-world policy settings.*

The forecast is not realistic: the average life expectancy of a population in the real world is extremely unlikely to be nearly 108 years. Nearly all individuals in a real-world population expire before 108.
  
However, the scenario itself is very extreme and unrealistic: the country would have nearly the lowest expenditure in the world but also nearly the highest proportion of physicians. Such a poor country could not train and support so many physicians with that level of expenditure. 

In a real world setting, models can extrapolate beyond what is realistic or even possible in the real world, and so care should be taken especially at extremes like this.

``` {r who6, message = FALSE, warning = FALSE}

#---------------
# Predict
#---------------

# create table with new total expense data
df_who_new_data2 <- tibble(TotExp = 14,
                           PropMD = 0.03)

# add predicted life expectancy and backtransform it
df_who_new_data2 <- df_who_new_data2 %>% 
  mutate(predicted_life_exp = predict(mod_life_interaction, 
                                         newdata = df_who_new_data2))

glimpse(df_who_new_data2)

# original dataset for context
# summarize
df_who_clean %>% 
  select(c(PropMD,TotExp)) %>% 
  summary()


```
## Problem 3-Retail Company Analyst
  
### 3.1 Inventory Cost
  
*Scenario: A retail company is planning its inventory strategy for the upcoming year. They expect to sell 110 units of a high-demand product. The storage cost is $3.75 per unit per year, and there is a fixed ordering cost of $8.25 per order. The company wants to minimize its total inventory cost.*

*Task: Using calculus, determine the optimal lot size (the number of units to order each time) and the number of orders the company should place per year to minimize total inventory costs. Assume that the total cost function is given by:*

$$ C(Q) = \frac{D}{Q} \cdot S + \frac{Q}{2} \cdot H$$
*D is the total demand (110 units).*
  
*Q is the order quantity.*
  
*S is the fixed ordering cost per order ($8.25).*
  
*H is the holding cost per unit per year ($3.75).*
  
To minimize cost, take the derivative of C(Q) (cost with respect to quantity), set it equal to zero, and solve for Q.
  
**Optimal lot size: 22**
  
**Optimal orders/year: 5**
  
``` {r retail1, message = FALSE, warning = FALSE}

D <- 110  # demand
S <- 8.25 # cost/order
H <- 3.75 # holding cost/year

# cost function
C <- function(Q) {(D/Q) * S + (Q/2) * H}

# derivative of C(Q) with respect to Q using Deriv package
C_prime <- Deriv(C, "Q")

# solve C'(Q) = 0 for optimal Q using rootSolve package
optimal_Q <- uniroot.all(C_prime, c(0.01, 1000))  # Avoid zero division
optimal_Q

# orders per year
optimal_order <- D/optimal_Q
optimal_order

# minimized cost
C(optimal_Q)

```
  
### 3.2 Revenue Maximization
  
*Scenario: A company is running an online advertising campaign. The effectiveness of the campaign, in terms of revenue generated per day, is modeled by the function:*
  
$$R(t) = -3150t^{-4} - 220t + 6530$$
  
*R(t) represents the revenue in dollars after t days of the campaign.*
  
*Task: Determine the time t at which the revenue is maximized by finding the critical points of the revenue function and determining which point provides the maximum value. What is the maximum revenue the company can expect from this campaign?*

To find the maximum revenue, I took the derivative of R(t) and set it equal to zero to find the critical points, then found the second derivative and checked its sign at that critical point. It was negative, so that point was a maximum.

**Maximum revenue = $5,912.09**

``` {r retail2, message = FALSE, warning = FALSE}

R <- function(t) {-3150 * t^(-4) - 220 * t + 6530}

# Derivative
R_prime <- Deriv(R, "t")

# set R' = 0 and solve using rootSolve package
t_critical <- uniroot.all(R_prime, c(0.5, 10))  # Avoid t=0 to prevent division by zero

# Second derivative
R_double_prime <- Deriv(R_prime, "t")

# Evaluate second derivative at the critical point
R_double_prime(t_critical) # negative, so this is a maximum

# Max revenue
max_revenue <- R(t_critical)
max_revenue

```

### 3.3 Demand Area Under Curve

*Scenario: A company sells a product at a price that decreases over time according to the linear demand function:*

$$P(x) = 2x - 9.3 $$

*Where: 
*P(x) is the price in dollars, and 
*x is the quantity sold.

*Task: The company is interested in calculating the total revenue generated by this product between two quantity levels, 
x1 = 2 and x2 = 5, where the price still generates sales. Compute the area under the demand curve between these two points, representing the total revenue generated over this range.*

The area under the demand curve is **-6.90** and represents the total value or willingness to pay for the range of quantities. The function implies the company must pay customers to take the product for most of this range (x < = 4.65). For example for P(5) = 0.7 but P(2) = -5.3. 

It is possible that the signs in the function are transposed. In a demand curve, as price goes up, demand should go down, or as x goes up, P(x) should go down.
  
Transposing the signs gives us that negative slope: $$P(x) = -2x + 9.3$$
  
The area under the curve (total value or willingness to pay) is now **6.90** which aligns with positive consumer value for the range of quantities.

Now P(2) = 5.30, P(4.65) = 0, and P(5) = -.70. Theoretically when price reaches 0, demand should be infinite, but this is a limitation of a linear demand function. 

To find total revenue between x = 2 and x = 4 (avoiding a zero or negative price), the derivative of P(x)*x can be taken.

``` {r retail3, message = FALSE, warning = FALSE}

# Price function
P <- function(x) {2 * x - 9.3}

# AUC using base R 
area <- integrate(P, lower = 2, upper = 5)$value

# Print result
area

# Revised price function
P2 <- function(x) {-2 * x + 9.3}

# AUC using base R 
area2 <- integrate(P2, lower = 2, upper = 5)$value

# Print result
area2

```

### 3.4 Profit Optimization
  
*Scenario: A beauty supply store sells flat irons, and the profit function associated with selling x flat irons is given by:*

$$\Pi(x) = x \ln (9x) - \frac{x^6}{6} $$

*Task: Use calculus to find the value of x that maximizes profit. Calculate the maximum profit that can be achieved and determine if this optimal sales level is feasible given market conditions.*

The flat irons are not a feasible item to sell in current conditions. The maximum profit is achieved at **x = 1.28** (one or two flat irons) and the **maximum profit is only $2.40**.

The term $\frac{-x^6}{6}$ increases rapidly with increases in quantity and quickly offsets any additional revenue.

``` {r retail4, message = FALSE, warning = FALSE}

# Profit function
Pi <- function(x) {x * log(9 * x) - (x^6) / 6}

# First derivative
Pi_prime <- Deriv(Pi, "x")

# Set Pi' = 0 and solve using rootSolve package
optimal_x <- uniroot.all(Pi_prime, c(0.1, 5))  # Avoid x = 0 due to log
optimal_x

# Second derivative
Pi_double_prime <- Deriv(Pi_prime, "x")

# Evaluate second derivative at the critical point
Pi_double_prime(optimal_x)  # Should be negative â‡’ maximum

# Max profit
max_profit <- Pi(optimal_x)
max_profit

```

### 3.5 Spending Behavior

*Scenario: A market research firm is analyzing the spending behavior of customers in a retail store. The spending behavior is modeled by the probability density function:*

$$f(x) = \frac{1}{6x} $$
 
*Where x represents spending in dollars.*

*Task: Determine whether this function is a valid probability density function over the interval [1, e^6]. If it is, calculate the probability that a customer spends between 1 and e^6 dollars.*

To determine if the function is a valid probability density function (PDF) over the interval, it must be non-negative and integrate to 1.

It is non-negative over the range because the whole set [1, e^6] is positive and $\frac{1}{6x}$ must be positive when x is positive.

It also integrates to 1 over this range, so **it is a valid PDF**.

The probability that a customer spends in that range, the entire PDF, is also **1**.

``` {r retail5, message = FALSE, warning = FALSE}

# PDF
pdf <- function(x) { 1 / (6 * x) }

# Define integration limits
pdf_a <- 1
pdf_b <- exp(6)

# Integrate to check validity
pdf_area <- integrate(pdf, lower = pdf_a, upper = pdf_b)
pdf_area$value  # Should be 1
```

### 3.6 Market Share Estimation

*Scenario: An electronics company is analyzing its market share over a certain period. The rate of market penetration is given by:*

$$\frac {dN}{dt} = \frac {500}{t^4 + 10} $$
*Where N(t) is the cumulative market share at time t.*

*Task: Integrate this function to find the cumulative market share N(t) after t days, given that the initial market share N(1) = 6530.* 
*What will the market share be after 10 days?*

After ten days, total cumulative market penetration will be 6597.54 or approximately **6580 customers**. The rate of change slows dramatically as t increases due to the t^4 in the denominator.

``` {r retail6, message = FALSE, warning = FALSE}

# define derivative dN/dt
dN_dt <- function(t) {500/(t^4 +10)}

# integrate from t= 1 to t = 10 (take value!)
mkt_integrated_area <- integrate(dN_dt, lower = 1, upper = 10)$value

# add initial market share at t = 1 
N_1 <- 6530
N_10 <- N_1 + mkt_integrated_area
N_10

```

## Problem 4: Business Optimization
  
### 4.1 Revenue and Cost
  
*Scenario: A companyâ€™s revenue from a product can be approximated by the function* $R(x) = e^x$ *where x is the number of units sold. The cost of production is given by* $C(x) = \ln(1+x)$. *The company wants to maximize its profit, defined as* $\Pi (x) = R(x) - C(x)$

*1. Approximate the Revenue Function: Use the Taylor Series expansion around x= 0 (Maclaurin series) to approximate the revenue function* $R(x) = e^x$ *up to the second degree. Explain why this approximation might be useful in a business context.*

The Maclaurin series for e^x at the second degree is $e^x \approx 1 + x + \frac{x^2}{2}$. 

This approximation is accurate for small values of x less than about 0.5 (see plot below). It would be easy to use for estimation especially when logs are impractical due to lack of skill/comfort level, lack of access to a computer/scientific calculator (which was more likely in the past), or to simplify complex optimization problems. 

``` {r business1, message = FALSE, warning = FALSE}

# define exact revenue function
bus_R_exact <- function(x) {exp(x)}

# define maclaurin series approximation function
bus_R_approx <- function(x) {1 + x + (x^2)/2}

# compare
bus_R_x_vals <- seq(0, 1, by = 0.1)

df_bus_R_compare <- data.frame(
  x = bus_R_x_vals,
  R_exact = bus_R_exact(bus_R_x_vals),
  R_approx = bus_R_approx(bus_R_x_vals),
  R_error = bus_R_approx(bus_R_x_vals) - bus_R_exact(bus_R_x_vals))

head(df_bus_R_compare, 11)

#plot
df_bus_R_compare_long <- pivot_longer(df_bus_R_compare, 
                                      cols = c("R_exact", "R_approx"), 
                                      names_to = "type", 
                                      values_to = "revenue")

plt_bus_R <- df_bus_R_compare_long %>% 
  ggplot(aes(x = x, y = revenue, color = type)) +
  geom_line(size = 1) +
  labs(title = "Exact vs Approximate Revenue Function",
       x = "x",
       y = "Revenue",
       color = "Function") +
  theme_minimal()

plt_bus_R

```

*2. Approximate the Cost Function: Similarly, approximate the cost function $C(x) = \ln (1+x)$using its Maclaurin series expansion up to the second degree. Discuss the implications of this approximation for decision-making in production.*
 
The Maclaurin series for $\ln(1+x)$ at the second degree is $\ln(1+x) \approx x - \frac{x^2}{2}$. 

This approximation is accurate for small values of x <0.25 (see plot below) but using it in production would greatly underestimate costs for values of x > 0.25 and should be avoided to avoid budget surprises. 

Note that the cost estimation function underestimates to a greater degree than the revenue estimation, so profit would also be underestimated for larger values of x. 

``` {r business2, message = FALSE, warning = FALSE}

# define exact cost function
bus_C_exact <- function(x) {log(1+x)}

# define maclaurin series approximation function
bus_C_approx <- function(x) {x - (x^2)/2}

# compare from 0 to 1
bus_C_x_vals <- seq(0, 1, by = 0.1)

df_bus_C_compare <- data.frame(
  x = bus_C_x_vals,
  C_exact = bus_C_exact(bus_C_x_vals),
  C_approx = bus_C_approx(bus_C_x_vals),
  C_error = bus_C_approx(bus_C_x_vals) - bus_C_exact(bus_C_x_vals))

head(df_bus_C_compare, 11)

#plot
df_bus_C_compare_long <- pivot_longer(df_bus_C_compare, 
                                      cols = c("C_exact", "C_approx"), 
                                      names_to = "type", 
                                      values_to = "cost")

plt_bus_C <- df_bus_C_compare_long %>% 
  ggplot(aes(x = x, y = cost, color = type)) +
  geom_line(size = 1) +
  labs(title = "Exact vs Approximate Cost Function",
       x = "x",
       y = "Cost",
       color = "Function") +
  theme_minimal()

plt_bus_C

```

*3. Linear vs. Nonlinear Optimization: Using the Taylor Series expansions, approximate the profit function* $\Pi (x) = R(x) - C(x)$
*Compare the optimization results when using the linear approximations versus the original nonlinear functions. What are the differences, and when might it be more appropriate to use the approximation?*

The profit estimation is accurate for most values of x and begins to underestimate around x ~ 0.9 (note that underestimating profit is conservative and not usually a bad thing). It would be appropriate to use for estimations overall, and in production or sensitive applications for values of x < 0.9.

``` {r business3, message = FALSE, warning = FALSE}

# combine rev and cost df
df_bus_P_compare <- merge(df_bus_C_compare,
                          df_bus_R_compare, 
                          by = "x")

# calculate P
df_bus_P_compare <-df_bus_P_compare %>% 
  mutate(P_exact = R_exact - C_exact,
         P_approx = R_approx - C_approx,
         P_error = P_approx - P_exact)

head(df_bus_P_compare, 10)

#plot
df_bus_P_compare_long <- pivot_longer(df_bus_P_compare, 
                                      cols = c("P_exact", "P_approx"), 
                                      names_to = "type", 
                                      values_to = "profit")

plt_bus_P <- df_bus_P_compare_long %>% 
  ggplot(aes(x = x, y = profit, color = type)) +
  geom_line(size = 1) +
  labs(title = "Exact vs Approximate Profit Function",
       x = "x",
       y = "Profit",
       color = "Function") +
  theme_minimal()

plt_bus_P

```

### 4.2 Financial Modeling
  
*Scenario: A financial analyst is modeling the risk associated with a new investment. The risk is proportional to the square root of the invested amount, modeled as* $f(x) = \sqrt x$ *, where x is the amount invested. However, to simplify calculations, the analyst wants to use a Taylor Series expansion to approximate this function for small investments.*

*1. Maclaurin Series Expansion: Derive the Taylor Series expansion around x=0 up to the second degree.*

x=0 is undefined for $\sqrt x$ because the derivative is undefined, so a Taylor series around x = 1 is used instead: $\sqrt x \approx 1 + \frac {x-1}{2}$

*2. Practical Application: Use the derived series to approximate the risk for small investment amounts (e.g., when x is small). Compare the approximated risk with the actual function values for small and moderate investments. Discuss when this approximation might be useful in financial modeling.*
  
The approximation is very close for most values of x between 0.5 and 1.5 with some deviation at the extremes as shown below. This would be very useful when simplifying calculations particularly around a typical normalized (x=1) investment amount.  
  
*3. Optimization Scenario: Suppose the goal is to minimize risk while maintaining a certain level of investment return. Using the Taylor Series approximation, suggest an optimal investment amount x that balances risk and return.*

No return function is given so we can't optimize for risk and return. To minimize risk alone, x would have to equal zero, because all levels of investment have risk. 

``` {r business4, message = FALSE, warning = FALSE}

# define exact risk function
bus_risk_exact <- function(x) {sqrt(x)}

# define taylor series approximation function
bus_risk_approx <- function(x) {1 + 0.5 * (x-1) - (1/8) * (x-1)^2}

# compare from 0.5 to 1.5
bus_risk_x_vals <- seq(0.5, 1.5, by = 0.1)

df_bus_risk_compare <- data.frame(
  x = bus_risk_x_vals,
  risk_exact = bus_risk_exact(bus_risk_x_vals),
  risk_approx = bus_risk_approx(bus_risk_x_vals),
  risk_error = bus_risk_approx(bus_risk_x_vals) -
               bus_risk_exact(bus_risk_x_vals))

head(df_bus_risk_compare, 11)

#plot
df_bus_risk_compare_long <- pivot_longer(df_bus_risk_compare, 
                                      cols = c("risk_exact",
                                               "risk_approx"), 
                                      names_to = "type", 
                                      values_to = "risk")

plt_bus_risk <- df_bus_risk_compare_long %>% 
  ggplot(aes(x = x, y = risk, color = type)) +
  geom_line(size = 1) +
  labs(title = "Exact vs Approximate Risk Function",
       x = "x",
       y = "Risk",
       color = "Function") +
  theme_minimal()

plt_bus_risk

```
  

### 4.3 Inventory Management
  
*Scenario: In a manufacturing process, the demand for a product decreases as the price increases, modeled by* $D(p) = 1 - p$ *, where p is the price. The cost associated with producing and selling the product is modeled as* $C(p) = e^p$ *. The company wants to maximize its profit, which is the difference between revenue and cost.*

*1. Taylor Series Expansion: Expand the cost function * $C(p) = e^p$ *into a Taylor Series around p=0 up to the second degree. Discuss why approximating the cost function might be useful in a pricing strategy.*
  
The Maclaurin series for e^p at the second degree is $e^p \approx 1 + p + \frac{p^2}{2}$. 

This approximation is accurate for values of p < 0.5 (see plot below). 

It might still be useful in a pricing strategy: since covering cost is only one component of pricing (with significant buffers for desired margin, strategic and market factors, and uncertainty built in), a good estimate like this can suffice and avoiding logs would simplify the optimization of the full pricing model.

``` {r inv1, message = FALSE, warning = FALSE}

# define exact cost function
inv_C_exact <- function(p) {exp(p)}

# define maclaurin series approximation function
inv_C_approx <- function(p) {1 + p + (p^2)/2}

# compare
inv_C_p_vals <- seq(0, 1, by = 0.1)

df_inv_C_compare <- data.frame(
  p = inv_C_p_vals,
  C_exact = inv_C_exact(inv_C_p_vals),
  C_approx = inv_C_approx(inv_C_p_vals),
  C_error = inv_C_approx(inv_C_p_vals) - inv_C_exact(inv_C_p_vals))

head(df_inv_C_compare, 11)

#plot
df_inv_C_compare_long <- pivot_longer(df_inv_C_compare, 
                                      cols = c("C_exact", "C_approx"), 
                                      names_to = "type", 
                                      values_to = "cost")

plt_inv_C <- df_inv_C_compare_long %>% 
  ggplot(aes(x = p, y = cost, color = type)) +
  geom_line(size = 1) +
  labs(title = "Exact vs Approximate Cost Function",
       x = "x",
       y = "Cost",
       color = "Function") +
  theme_minimal()

plt_inv_C

```
  
*2. Approximating Profit: Using the Taylor Series expansion, approximate the profit function* $\Pi(p) = pD(p) - C(p)$ *. Compare the results when using the original nonlinear cost function versus the approximated cost function. What differences do you observe, and when might the approximation be sufficient?*

The approximated profit is largely accurate for small values of p < 0.5 but overestimates significantly above that. 
  
Overestimating profit is a bigger problem than underestimating, but it could still be useful in planning with an adjustment for error.

``` {r inv2, message = FALSE, warning = FALSE}

glimpse(df_inv_C_compare)

# calculate D and P_exact, P_est
df_inv_P_compare <- df_inv_C_compare %>% 
  mutate(D = 1-p,
         Rev = p * D,
         P_exact = Rev - C_exact,
         P_approx = Rev - C_approx,
         P_error = P_approx - P_exact)

head(df_inv_P_compare, 11)

#plot
df_inv_P_compare_long <- pivot_longer(df_inv_P_compare, 
                                      cols = c("P_exact", "P_approx"), 
                                      names_to = "type", 
                                      values_to = "profit")

plt_inv_P <- df_inv_P_compare_long %>% 
  ggplot(aes(x = p, y = profit, color = type)) +
  geom_line(size = 1) +
  labs(title = "Exact vs Approximate Profit Function",
       x = "x",
       y = "Profit",
       color = "Function") +
  theme_minimal()

plt_inv_P

```
  
*3. Pricing Strategy: Based on the Taylor Series approximation, suggest a pricing strategy that could maximize profit. Explain how the Taylor Series approximation helps in making this decision.*
  
Based on this model, I would not sell this product because it is unprofitable at any price. Revenue is maximized at p = 0.5 in the table below, but cost grows exponentially since $C = e^p$ and outpaces revenue $R = p * (1-p)$. 

``` {r inv3, message = FALSE, warning = FALSE}

head(df_inv_P_compare, 11)
```

### 4.4 Economic Forecasting
  
*Scenario: An economist is forecasting economic growth, which can be modeled by the logarithmic function * $G(x) = \ln(1 + x)$ *where x represents investment in infrastructure. The government wants to predict growth under different levels of investment. *

*1. Maclaurin Series Expansion: Derive the Maclaurin Series expansion of up to the second degree. Explain the significance of using this approximation for small values of x in economic forecasting.*

The Maclaurin series would be $\ln(1 + x) \approx x - \frac{x^2}{2}$

It would be useful to simplify modeling growth estimates for small values of x. 

*2. Approximation of Growth: Use the Taylor Series to approximate the growth for small investments. Compare this approximation with the actual growth function. Discuss the accuracy of the approximation for different ranges of x.*

This approximation is accurate for small values of x < 0.25 and would be useful for modeling the impact of small amounts of investment. It does increasingly underestimate growth for larger values of x, which might be conservative and acceptable for planning.

``` {r econ1, message = FALSE, warning = FALSE}

# define exact growth function
econ_growth_exact <- function(x) {log(1+x)}

# define taylor series approximation function
econ_growth_approx <- function(x) {x - (x^2)/2}

# compare from 0 to 1
econ_growth_x_vals <- seq(0, 1, by = 0.1)

df_econ_growth_compare <- data.frame(
  x = econ_growth_x_vals,
  growth_exact = econ_growth_exact(econ_growth_x_vals),
  growth_approx = econ_growth_approx(econ_growth_x_vals),
  growth_error = econ_growth_approx(econ_growth_x_vals) -
                 econ_growth_exact(econ_growth_x_vals))

head(df_econ_growth_compare, 11)

#plot
df_econ_growth_compare_long <- pivot_longer(df_econ_growth_compare, 
                                      cols = c("growth_exact",
                                               "growth_approx"), 
                                      names_to = "type", 
                                      values_to = "growth")

plt_econ_growth <- df_econ_growth_compare_long %>% 
  ggplot(aes(x = x, y = growth, color = type)) +
  geom_line(size = 1) +
  labs(title = "Exact vs Approximate Growth Function",
       x = "x",
       y = "Growth",
       color = "Function") +
  theme_minimal()

plt_econ_growth

```



*3. Policy Recommendation: Using the approximation, recommend a level of investment that could achieve a target growth rate. Discuss the limitations of using Taylor Series approximations for such policy recommendations.*

Using the table below, we can make recommendations to achieve targeted growth rates. For example, for a targeted growth rate of 25%, we would recommend a normalized investment amount of ~0.3.

The limitations of Taylor series approximations are that they are most accurate near the expansion point (here x = 0). In this case, higher values of x would overestimate the investment needed to achieve a targeted growth rate.

``` {r econ2, message = FALSE, warning = FALSE}

head(select(df_econ_growth_compare, x, growth_approx), 11)
```

## 5. Profit, Cost, & Pricing
  
### 5.1 Profit Maximization
*Scenario: A company produces two products, A and B. The profit function for the two products is given by:* $\Pi(x,y) = 30x - 2x^2 - 3xy + 24y - 4y^2$

*Where:*
$x$ *is the quantity of Product A produced and sold*
$y$ *is the quantity of Product B produced and sold*
$\Pi(x,y)$ *is the profit in dollars*

*1. Find all local maxima, local minima, and saddle points for the profit function.* 
*2. Write your answer(s) in the form* $(x, y, \Pi(x,y))$ *. Separate multiple points with a comma.*

**The critical point (7.30, 0.26, 112.7) is a local maximum.**

*Discussion: Discuss the implications of the results for the companyâ€™s production strategy. Which production levels maximize profit, and what risks are associated with the saddle points?*
  
Since this is the only critical point and it's a local maximum, the company should produce approximately 7.3 units of A and 0.26 units of B to maximize profit. Note that the model is continuous so the company may need to round units.
  
A saddle point on the plane would pose risks to optimization because it is sloped both up and down at that point: even small shifts in inputs could increase profit in one direction and decrease it in another at the same time.

``` {r profit1, message = FALSE, warning = FALSE}

# define the profit function
profit <- function(v) {x <- v[1]
                       y <- v[2]
                       30*x - 2*x^2 - 3*x*y + 24*y - 4*y^2}

# define first partial derivative function - Gradient using numDeriv package
grad_profit <- function(v) {grad(profit, v)}

# define second partial derivative function - Hessian
hess_profit <- function(v) {hessian(profit, v)}

# find first partial derivative vector using rootSolve package
prof_critical_point <- multiroot(f = grad_profit, start = c(1, 1))$root
prof_critical_point

# Evaluate profit at the critical point
profit_value <- profit(prof_critical_point)

# Evaluate Hessian at critical point
H_prof <- hess_profit(prof_critical_point)

# Determine type of critical point using Hessian determinant
det_H_prof <- det(H_prof)
trace_H_prof <- sum(diag(H_prof))

if (det_H_prof > 0 && trace_H_prof < 0) {
  type <- "local maximum"
} else if (det_H_prof > 0 && trace_H_prof > 0) {
  type <- "local minimum"
} else if (det_H_prof < 0) {
  type <- "saddle point"
} else {
  type <- "inconclusive"
}

# Output
list(
  point = round(c(x = prof_critical_point[1], 
                  y = prof_critical_point[2]), 
                2),
  profit = round(profit_value, 2),
  type = type
)

```
  
### 5.2 Pricing Strategy
  
*Scenario: A supermarket sells two competing brands of a product: Brand X and Brand Y. The store manager estimates that the demand for these brands depends on their prices, given by the functions:*

*Demand for Brand X:* $D_x(x,y) = 120 - 15x + 10y$ 
*Demand for Brand Y:* $D_y(x,y) = 80 + 5x - 20y$
*Where:*
$x$ *is the price of Brand X in dollars.*
$y$ *is the price of Brand Y in dollars.*

*1. Revenue Function: Find the revenue function*$R(x,y)$ *for both brands combined.*

Revenue for X:
$$R_x = x * D_x = x(120 - 15x + 10y)$$
Revenue for Y:
$$R_y = y * D_y = y(80 + 5x - 20y)$$
Combined Revenue: 
$$R_{(x,y)} = R_x + R_y = x(120 - 15x + 10y) + y(80 + 5x - 20y)$$
$$R_{(x,y)} = 120x - 15x^2 + 10xy + 80y + 5xy - 20y^2$$
$$R_{(x,y)} = - 15x^2 - 20y^2 + 15xy + 120x + 80y$$

*2. Optimal Pricing: Determine the prices x and y that maximize the storeâ€™s total revenue. Are there any saddle points to consider in the pricing strategy?*
  
**Total revenue is maximized at $58.51 when the price of X = $6.15 and the price of Y = $4.31**

This is the only critical point and it is a local maximum so there are no saddle points in this plane to consider.
  
*Discussion: Explain the significance of the optimal pricing strategy and how it can be applied in a competitive retail environment.*
  
This optimal pricing model maximizes revenue while taking the interaction between competing (or complimentary) products into account. In a competitive environment, this store can now adjust each item's price to balance this interaction for maximum revenue. 
  
Note that similar price optimization models might optimize mix for profit, volume, etc. to support the retailer's particular strategic objective.
  
``` {r market1, message = FALSE, warning = FALSE}

# define the combined revenue function
mkt_revenue <- function(v) {
  x <- v[1]
  y <- v[2]
  -15 * x^2 - 20 * y^2 + 15 * x * y + 120 * x + 80 * y}

# define first partial derivative function - Gradient using numDeriv package
mkt_grad_revenue <- function(v) {grad(mkt_revenue, v)}

# define second partial derivative function - Hessian
mkt_hess_revenue <- function(v) {hessian(mkt_revenue, v)}

# find first partial derivative vector using rootSolve package
mkt_critical_point <- multiroot(f = mkt_grad_revenue, 
                                start = c(1, 1))$root
mkt_critical_point

# Evaluate profit at the critical point
mkt_revenue_value <- profit(mkt_critical_point)

# Evaluate Hessian at critical point
H_mkt <- hess_profit(mkt_critical_point)

# Determine type of critical point using Hessian determinant
det_H_mkt <- det(H_mkt)
trace_H_mkt <- sum(diag(H_mkt))

if (det_H_mkt > 0 && trace_H_mkt < 0) {
  mkt_type <- "local maximum"
} else if (det_H_mkt > 0 && trace_H_mkt > 0) {
  mkt_type <- "local minimum"
} else if (det_H_mkt < 0) {
  mkt_type <- "saddle point"
} else {
  mkt_type <- "inconclusive"
}

# Output
list(
  point = round(c(x = mkt_critical_point[1], 
                  y = mkt_critical_point[2]), 
                2),
  revenue = round(mkt_revenue_value, 2),
  type = mkt_type
)

```

### 5.3 Cost Minimization
  
*Scenario: A manufacturing company operates two plants, one in New York and one in Chicago. The company needs to produce a total of 200 units of a product each week. The total weekly cost of production is given by:* $$C(x,y) = \frac {1}{8}x^2 + \frac{1}{10}y^2 + 12x + 18y +1500$$
  
*Where:*
$x$ *is the number of units produced in New York.*
$y$ *is the number of units produced in Chicago.*
$C(x,y)$ *is the total cost in dollars.*

*Determine how many units should be produced in each plant to minimize the total weekly cost.*

Because there is a linear constraint x+y=200, I used the Rsolnp package to solve the constrained optimization problem.

**Optimal units:**
**New York: 102** 
**Chicago: 88**

*What is the minimized total cost, and how does the distribution of production between the two plants affect overall efficiency?*

**Minimized total cost = $6,749**

The distribution of production can minimize cost because the plants have different cost structures. 

Chicago has a higher baseline cost but New York grows faster as production ramps up. 

*Discussion: Discuss the benefits of this cost-minimization strategy and any practical considerations that might influence the allocation of production between the two plants.*

The production cost minimization strategy may help maximize profitability for these products. In practice, location-based constraints like plant capacity, suppliers, labor agreements, etc., may constrain production further. Also, additional costs like transportation and warehousing may impact the selection of a site to maximize profitability.

``` {r manu1, message = FALSE, warning = FALSE}

# define constraint
manu_constraint <- function(v) {
  x <- v[1]
  y <- v[2]
  x + y}

# define the combined cost function
manu_cost <- function(v) {
  x <- v[1]
  y <- v[2]
  (1/8)*x^2 + (1/10)*y^2 + 12*x + 18*y + 1500}

# solve using solnp in Rsolnp package
manu_result <- solnp(
  pars = c(100, 100),
  fun = manu_cost,
  eqfun = manu_constraint,
  eqB = 200
)

# extract solution ROUNDED
manu_optimal <- round(manu_result$pars, 0)
manu_min_cost <- round(manu_cost(manu_optimal), 0)

# output
list(
  units = c(New_York = manu_optimal[1], Chicago = manu_optimal[2]),
  total_cost = manu_min_cost
)

```

### 5.4 Marketing Mix
  
*Scenario: A company is launching a marketing campaign that involves spending on online ads* $x$ *and television ads* $y$. *The effectiveness of the campaign, measured in customer reach, is modeled by the function:*
$$E(x,y) = 500x + 700y - 5x^2 - 10xy - 8y^2$$
*Where:*
$x$ *is the amount spent on online ads (in thousands of dollars).
$y$ *is the amount spent on television ads (in thousands of dollars).
$E(x,y)$ *is the estimated customer reach.

*1. Find the spending levels for online and television ads that maximize customer reach.*

** Online: $16,670**
** TV: $33,330**
  
** Optimized reach: 15,833 customers**

*2. Identify any saddle points and discuss how they could affect the marketing strategy.*

No saddle points were found. Saddle points would indicate unstable regions in the plane where small changes in inputs could increase reach in one area while decreasing in another resulting in risk.

*Discussion: Explain how the results can be used to allocate the marketing budget effectively and what the company should consider if it encounters saddle points in the optimization.*

The results show an optimal allocation of advertising dollars that can be used in budget planning. Constraints could be added to the model to reflect actual budget constraints if needed. If saddle points had been present, the company would need to be cautious to avoid unstable regions of the budget space.

``` {r mkting1, message = FALSE, warning = FALSE}

# define the effectiveness function
mktg_eff <- function(v) {
  x <- v[1]
  y <- v[2]
  500 * x + 700 * y - 5 * x^2 - 10 * x * y - 8 * y^2
}

# gradient and hessian functions
mktg_grad <- function(v) { grad(mktg_eff, v) }
mktg_hess <- function(v) { hessian(mktg_eff, v) }

# find critical point first partial derivative vector using rootSolve package
mktg_critical_point <- multiroot(f = mktg_grad, start = c(10, 10))$root
mktg_reach <- mktg_eff(mktg_critical_point)

# Evaluate Hessian at critical point
H_mktg <- mktg_hess(mktg_critical_point)
det_H_mktg <- det(H_mktg)
trace_H_mktg <- sum(diag(H_mktg))

if (det_H_mktg > 0 && trace_H_mktg < 0) {
  mktg_type <- "local maximum"
} else if (det_H_mktg > 0 && trace_H_mktg > 0) {
  mktg_type <- "local minimum"
} else if (det_H_mktg < 0) {
  mktg_type <- "saddle point"
} else {
  mktg_type <- "inconclusive"
}

# output
list(
  point = round(c(x = mktg_critical_point[1], 
                  y = mktg_critical_point[2]), 2),
  reach = round(mktg_reach, 2),
  type = mktg_type
)

```